import json
import sys

##### Step 1: Construct the configuration file #####
# See work/config.json for the configuration file format
#   Note: This config file can be generated by the Relevant Text Extraction Agent (see ../relevant_text_extraction/parse_results.py)

##### Step 2: Create the batch input file #####
# Execute this script to create the OpenAPI batch API input file (work/property_selection_input.jsonl)

# Configurations
work_dir = sys.argv[1] if len(sys.argv) > 1 else "work"
model = sys.argv[2] if len(sys.argv) > 2 else "gpt-5-2025-08-07"

config_file = f"{work_dir}/config.json"
batch_input_file = f"{work_dir}/property_selection_input.jsonl"

# Read the configuration file
with open(config_file, "r") as f:
    config = json.load(f)

# Property selection
system_instruction = "You are an expert in formal verification of System-on-Chip (SoC) designs. You will be provided with (1) a textual description of a specific mechanism (e.g., data transfer, device reset, ...) of an on-chip communication protocol, and (2) a list of candidate signal interaction rules. Note that the candidate rules will later be translated to SVA properties to verify actual hardware designs of the protocol. Your task is to extract the candidate rules that best describe the target mechanism based on the textual description. Do not include rules irrelevant to the target mechanism. Provide the extracted rules in a JSON list format, without any additional commentary or explanation."

# Create the batch in jsonl format
requests = []
for entry in config:
    protocol = entry["protocol"]
    textual_description = entry["textual_description"]
    batch_size = entry["batch_size"] if "batch_size" in entry else 0

    with open(entry["candidates"], "r") as f:
        candidates = json.load(f)
    
    if batch_size > 0:
        # Break down into smaller batches
        candidates_batches = [candidates[i:i + batch_size] for i in range(0, len(candidates), batch_size)]
    elif type(candidates[0]) == list:
        # Already batched
        candidates_batches = candidates
    else:
        # No batch -> Single batch
        candidates_batches = [candidates]
    # print(f"Protocol: {protocol}, total candidates: {len(candidates)}, batches: {len(candidates_batches)}")

    for i in range(len(candidates_batches)):
        requests.append({
            "custom_id": f"{protocol}: batch {i}" if len(candidates_batches) > 1 else protocol,
            "method": "POST",
            "url": "/v1/responses",
            "body": {
                "model": model,
                "instructions": system_instruction,
                "input": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "input_text",
                                "text": f"Textual description of {protocol}:\n==========\n{textual_description}\n==========\n\nCandidate rules:\n{json.dumps(candidates_batches[i], indent=4)}"
                            }
                        ]
                    }
                ]
            }
        })

# Write to the jsonl file
with open(batch_input_file, "w") as f:
    for req in requests:
        f.write(json.dumps(req) + "\n")


##### Step 3: Query the LLM and retrieve results #####

# Terminal command: Upload the batch input file
# curl https://api.openai.com/v1/files -H "Authorization: Bearer $OPENAI_API_KEY" -F purpose="batch" -F file="@property_selection_input.jsonl"

# Terminal command: Create the batch
# curl https://api.openai.com/v1/batches -H "Authorization: Bearer $OPENAI_API_KEY" -H "Content-Type: application/json" -d '{ "input_file_id": "<input_file_id>", "endpoint": "/v1/responses", "completion_window": "24h" }'

# Terminal command: Retrieve the results
# curl https://api.openai.com/v1/files/<file-xyz123>/content -H "Authorization: Bearer $OPENAI_API_KEY" > property_selection_output.jsonl
